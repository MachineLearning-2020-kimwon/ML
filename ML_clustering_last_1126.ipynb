{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "\n",
    "d1 = pd.read_csv('Indicators.csv')\n",
    "d2 = pd.read_csv('Country.csv')\n",
    "\n",
    "data = d1.copy()\n",
    "df_2 = d2.copy()\n",
    "\n",
    "# 처음 dataframe만들 때만 사용\n",
    "# df : indicators.csv\n",
    "# indicator : 사용할 indicator\n",
    "# year : 선택할 year\n",
    "def make_df(df,indicator,year):\n",
    "    df_1 = df[df['Year'] == year]\n",
    "    df_1 = df_1[df_1['IndicatorName'] == indicator]\n",
    "    df_1 = df_1[['CountryCode','Value']]\n",
    "    return df_1\n",
    "\n",
    "\n",
    "# data : indicators.csv\n",
    "# df : merge하기전 dataframe\n",
    "# indicator : 새로 merge하고싶은 indicator\n",
    "# year : mearge 하기 위한 year 선택\n",
    "# df_2 : 새로 merge하기위해 만들어진 dataframe\n",
    "def data_merge(df,indicator,year):\n",
    "    df_2 = data[data['Year'] == year]\n",
    "    df_2 = df_2[df_2['IndicatorName'] == indicator]\n",
    "    df_2 = df_2[['CountryCode','Value']]\n",
    "    df_merge = pd.merge(df, df_2, on='CountryCode', how='outer')\n",
    "    #df_merge.columns = ['CountryCode', indicator+'_value',indicator2+'_value']\n",
    "    #df_merge['Year'] = year\n",
    "    return df_merge\n",
    "\n",
    "#'Birth rate, crude (per 1,000 people)'\n",
    "# Using indicators list\n",
    "indi_list = ['Life expectancy at birth, total (years)','Age dependency ratio, old (% of working-age population)','Birth rate, crude (per 1,000 people)']\n",
    "# 1st DataFrame\n",
    "#df1 = make_df(data,indi_list[0],2000)\n",
    "# Merge 1 indicator\n",
    "#merge_1 = data_merge(df1,indi_list[1],2000)\n",
    "# Merge 1 indicator \n",
    "#merge_2 = data_merge(merge_1,indi_list[2],2000)\n",
    "\n",
    "#merge_2.columns = ['CountryCode',indi_list[0],indi_list[1],indi_list[2]]\n",
    "\n",
    "#merge_last = merge_2.copy()\n",
    "\n",
    "# Check the Null value\n",
    "def check_isnan_list(df,col):\n",
    "    null_list = []\n",
    "    for i in range(len(df)):\n",
    "        if math.isnan(df[col][i]):\n",
    "            null_list.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return null_list\n",
    "\n",
    "\n",
    "\n",
    "# Check the Null value\n",
    "def check_isnan_dict(df,col):\n",
    "    null_list = []\n",
    "    for i in range(len(df)):\n",
    "        if math.isnan(df[col][i]):\n",
    "            null_list.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    null_dict = dict()\n",
    "    for country in df['CountryCode'][null_list]:\n",
    "        for index in range(len(df_2)):\n",
    "            if df_2['CountryCode'][index] == country:\n",
    "                region = df_2['Region'][index]\n",
    "                null_dict[country] = region\n",
    "    return null_dict\n",
    "\n",
    "# df1 : Indicator.csv , df2 : Country.csv\n",
    "def Check_country_IncomeGroup(df1,df2,group,indicator,year):\n",
    "    country_group = df2[df2['IncomeGroup'] == group]['CountryCode']\n",
    "    data_group = df1[df1['CountryCode'].isin(country_group)]\n",
    "    return data_group[(data_group['IndicatorName'] == indicator) & (data_group['Year'] == year)]\n",
    "\n",
    "# df1 : Indicator.csv , df2 : Country.csv\n",
    "def Check_country_region(df1,df2,region,indicator,year):\n",
    "    country_region = df2[df2['Region'] == region]['CountryCode']\n",
    "    data_region = df1[df1['CountryCode'].isin(country_region)]\n",
    "    return data_region[(data_region['IndicatorName'] == indicator) & (data_region['Year'] == year)]\n",
    "\n",
    "# df1 : Indicator.csv , df2 : Country.csv\n",
    "def Check_CurrencyUnit(df1,df2,currencyunit,indicator,year):\n",
    "    unit = df2[df2['CurrencyUnit'] == currencyunit]['CountryCode']\n",
    "    data_unit = df1[df1['CountryCode'].isin(unit)]\n",
    "    return data_unit[(data_unit['IndicatorName'] == indicator) & (data_unit['Year'] == year)]\n",
    "\n",
    "# Check the Null value\n",
    "# df : merge가 끝난 dataframe\n",
    "# col : null값을 채우기 위한 indicator value\n",
    "# year : 해당 데이터셋의 year\n",
    "# method : null값을 채우기 위한 기준\n",
    "def change_null_value(df,col,year,method):\n",
    "    null_list = []\n",
    "    for i in range(len(df)):\n",
    "        if math.isnan(df[col][i]):\n",
    "            null_list.append(i)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    null_dict = dict()\n",
    "    if method =='Region':\n",
    "        for country in df['CountryCode'][null_list]:\n",
    "            for index in range(len(df_2)):\n",
    "                if df_2['CountryCode'][index] == country:\n",
    "                    region = df_2[method][index]\n",
    "                    null_dict[country] = region\n",
    "        keys = list(null_dict.keys())        \n",
    "        for key,null_value in zip(keys,null_list):\n",
    "            new_value_data = Check_country_region(data,df_2,null_dict[key],col,year)\n",
    "            new_value = np.average(new_value_data['Value'])\n",
    "            #dff = dff.T\n",
    "            #dff.at[col,null_value] = new_value\n",
    "            #dff = dff.T\n",
    "            df[col][null_value] = new_value\n",
    "        return df\n",
    "    \n",
    "    elif method =='IncomeGroup':\n",
    "        for country in df['CountryCode'][null_list]:\n",
    "            for index in range(len(df_2)):\n",
    "                if df_2['CountryCode'][index] == country:\n",
    "                    group = df_2[method][index]\n",
    "                    null_dict[country] = group\n",
    "        keys = list(null_dict.keys())        \n",
    "        for key,null_value in zip(keys,null_list):\n",
    "            new_value_data = Check_country_IncomeGroup(data,df_2,null_dict[key],col,year)\n",
    "            new_value = np.average(new_value_data['Value'])\n",
    "            df[col][null_value] = new_value       \n",
    "        return df\n",
    "    \n",
    "    elif method == 'CurrencyUnit':\n",
    "        for country in df['CountryCode'][null_list]:\n",
    "            for index in range(len(df_2)):\n",
    "                if df_2['CountryCode'][index] == country:\n",
    "                    unit = df_2[method][index]\n",
    "                    null_dict[country] = unit\n",
    "        keys = list(null_dict.keys())        \n",
    "        for key,null_value in zip(keys,null_list):\n",
    "            new_value_data = Check_CurrencyUnit(data,df_2,null_dict[key],col,year)\n",
    "            new_value = np.average(new_value_data['Value'])\n",
    "            df[col][null_value] = new_value       \n",
    "        return df\n",
    "    \n",
    "def merge_null_change(data,year,method):\n",
    "    # Using indicators list\n",
    "    indi_list = ['Life expectancy at birth, total (years)','Age dependency ratio, old (% of working-age population)','Birth rate, crude (per 1,000 people)']\n",
    "    # 1st DataFrame\n",
    "    df1 = make_df(data,indi_list[0],year)\n",
    "    # Merge 1 indicator\n",
    "    merge_1 = data_merge(df1,indi_list[1],year)\n",
    "    # Merge 1 indicator \n",
    "    merge_2 = data_merge(merge_1,indi_list[2],year)\n",
    "    merge_2.columns = ['CountryCode',indi_list[0],indi_list[1],indi_list[2]]\n",
    "    merge_last = merge_2.copy()\n",
    "\n",
    "    df_method= merge_last.copy()\n",
    "    for i in range(3):\n",
    "        df_method = change_null_value(df_method,indi_list[i],year,method)\n",
    "    return df_method\n",
    "\n",
    "## DBSCAN ##\n",
    "    # Make the list for using compare the result for various parameter combinations\n",
    "def Show_DBSCAN(df):\n",
    "    eps = [0.001,0.005,0.01,0.02,0.05,0.1,0.2,0.5]\n",
    "    min_samples= [3,5,10,15,20,30,50,100]\n",
    "    for i in eps:\n",
    "        for j in min_samples:\n",
    "            dbscan = DBSCAN(eps = i, min_samples = j).fit(df)\n",
    "            labels_dbscan = dbscan.labels_\n",
    "            dbscan_cluster_num = labels_dbscan.max() +2\n",
    "            plt.figure(figsize=(9,9))\n",
    "            plt.scatter(df['P1'],df['P2'],c=labels_dbscan)\n",
    "            plt.title(\"DBSCAN Scatter - eps: {}, min_samples: {}, cluster_num: {}\".format(i,j,dbscan_cluster_num))\n",
    "            plt.show()\n",
    "            \n",
    "## KMeans ##\n",
    "\n",
    "    # Make the list for using compare the result for various parameter combinations\n",
    "def Show_KMeans(df):\n",
    "    n_clusters = [2,3,4,5,6]\n",
    "    max_iter= [50,100,200,300]\n",
    "    for i in n_clusters: \n",
    "        for j in max_iter: \n",
    "                # Make the Kmeans instance and fit\n",
    "            Kmeans = KMeans(n_clusters=i, max_iter = j).fit(df)\n",
    "                # Make to list for kmeans labels value\n",
    "            labels_kmeans = Kmeans.labels_\n",
    "                # Count the cluster num\n",
    "            kmeans_cluster_num = labels_kmeans.max() + 1\n",
    "        \n",
    "                ## Plot the Scatter each of parameters ##\n",
    "            plt.figure(figsize =(9, 9)) \n",
    "            plt.scatter(df['P1'], df['P2'], c = labels_kmeans)   \n",
    "                # Building the title\n",
    "            plt.title(\"KMeans Scatter - n_clusters: {}, max_iter: {}, cluster_num: {}\".format(i,j,kmeans_cluster_num))\n",
    "            plt.show() \n",
    "            \n",
    "## GMM ##\n",
    "\n",
    "    # Make the list for using compare the result for various parameter combinations\n",
    "def Show_GMM(df):\n",
    "    n_components = [2,3,4,5,6]\n",
    "    max_iter= [50,100,200,300]\n",
    "    for i in n_components: \n",
    "        for j in max_iter:\n",
    "                # Make the GMM instance and fit\n",
    "            gmm = GMM(n_components= i , max_iter = j).fit(df)\n",
    "                # Make to list for GMM labels value\n",
    "            labels_gmm = gmm.predict(df)\n",
    "                # Count the cluster num\n",
    "            gmm_cluster_num = labels_gmm.max() + 1\n",
    "        \n",
    "                ## Plot the Scatter each of parameters ##\n",
    "            plt.figure(figsize =(9, 9)) \n",
    "            plt.scatter(df['P1'], df['P2'], c = labels_gmm)   \n",
    "                # Building the title\n",
    "            plt.title(\"GMM Scatter - n_components: {},max_iter: {}, cluster_num: {}\".format(i,j,gmm_cluster_num))\n",
    "            plt.show()             \n",
    "\n",
    "\n",
    "def plotSilhouette(data, cluster_label):\n",
    "    cluster_labels = np.unique(cluster_label)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_samples(data, cluster_label, metric='euclidean')\n",
    "    y_ax_lower, y_ax_upper = 0,0\n",
    "    yticks = []\n",
    "    \n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[cluster_label ==c]\n",
    "        c_silhouette_vals.sort()\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(i/n_clusters)\n",
    "        \n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0,\n",
    "                    edgecolor = 'none', color = color)\n",
    "        yticks.append((y_ax_lower + y_ax_upper)/2)\n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "    silhoutte_avg = np.mean(silhouette_vals)\n",
    "    #plt.figure(figsize=(9,9))\n",
    "    plt.axvline(silhoutte_avg, color='red', linestyle='--')\n",
    "    plt.yticks(yticks, cluster_labels+1)\n",
    "    plt.ylabel('cluster')\n",
    "    plt.xlabel('silhoueete value')\n",
    "  #  plt.title(\" Silhouette - eps: {}, min_samples: {}, cluster_num: {}\".format(i,j,dbscan_cluster_num))\n",
    "    plt.show()\n",
    "\n",
    "data_1960_region = merge_null_change(data,1960,'Region')\n",
    "data_1980_region = merge_null_change(data,1980,'Region')\n",
    "data_2000_region = merge_null_change(data,2000,'Region')\n",
    "data_2010_region = merge_null_change(data,2010,'Region')\n",
    "\n",
    "data_1960_group = merge_null_change(data,1960,'IncomeGroup')\n",
    "data_1980_group = merge_null_change(data,1980,'IncomeGroup')\n",
    "data_2000_group = merge_null_change(data,2000,'IncomeGroup')\n",
    "data_2010_group = merge_null_change(data,2010,'IncomeGroup')\n",
    "\n",
    "data_1960_unit = merge_null_change(data,1960,'CurrencyUnit')\n",
    "data_1980_unit = merge_null_change(data,1980,'CurrencyUnit')\n",
    "data_2000_unit = merge_null_change(data,2000,'CurrencyUnit')\n",
    "data_2010_unit = merge_null_change(data,2010,'CurrencyUnit')\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data_region = pd.concat([data_1960_region,data_1980_region,data_2000_region,data_2010_region])\n",
    "data_region['CountryCode'] = encoder.fit_transform(data_region['CountryCode'])\n",
    "\n",
    "data_group = pd.concat([data_1960_group,data_1980_group,data_2000_group,data_2010_group])\n",
    "data_group['CountryCode'] = encoder.fit_transform(data_group['CountryCode'])\n",
    "\n",
    "data_unit = pd.concat([data_1960_unit,data_1980_unit,data_2000_unit,data_2010_unit])\n",
    "data_unit = data_unit.dropna(axis=0)\n",
    "data_unit['CountryCode'] = encoder.fit_transform(data_unit['CountryCode'])\n",
    "\n",
    "## Preprocessing the data ##\n",
    "\n",
    "# Make the MinMaxScaler instance and StandardScaler instance\n",
    "# Compare the result by using two Scaler\n",
    "scaler_mm = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Fit the DataFrame for each of Scaler\n",
    "df_scaled_mm_region = scaler_mm.fit_transform(data_region)\n",
    "df_scaled_std_region = scaler_std.fit_transform(data_region)\n",
    "\n",
    "# Fit the DataFrame for each of Scaler\n",
    "df_scaled_mm_group = scaler_mm.fit_transform(data_group)\n",
    "df_scaled_std_group = scaler_std.fit_transform(data_group)\n",
    "\n",
    "# Fit the DataFrame for each of Scaler\n",
    "df_scaled_mm_unit = scaler_mm.fit_transform(data_unit)\n",
    "df_scaled_std_unit = scaler_std.fit_transform(data_unit)\n",
    "\n",
    "\n",
    "# Change the normalized and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_normalized_mm_region = normalize(df_scaled_mm_region)\n",
    "df_normalized_mm_region = pd.DataFrame(df_normalized_mm_region)\n",
    "\n",
    "df_normalized_std_region = normalize(df_scaled_std_region)\n",
    "df_normalized_std_region = pd.DataFrame(df_normalized_std_region)\n",
    "\n",
    "# Change the normalized and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_normalized_mm_group = normalize(df_scaled_mm_group)\n",
    "df_normalized_mm_group = pd.DataFrame(df_normalized_mm_group)\n",
    "\n",
    "df_normalized_std_group = normalize(df_scaled_std_group)\n",
    "df_normalized_std_group = pd.DataFrame(df_normalized_std_group)\n",
    "\n",
    "\n",
    "# Change the normalized and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_normalized_mm_unit = normalize(df_scaled_mm_unit)\n",
    "df_normalized_mm_unit = pd.DataFrame(df_normalized_mm_unit)\n",
    "\n",
    "df_normalized_std_unit = normalize(df_scaled_std_unit)\n",
    "df_normalized_std_unit = pd.DataFrame(df_normalized_std_unit)\n",
    "\n",
    "\n",
    "## Reducing the dimensionality of the data by using the PCA ##\n",
    "\n",
    "# Make the PCA instance\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit the data and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_principal_mm_region = pca.fit_transform(df_normalized_mm_region)\n",
    "df_principal_mm_region = pd.DataFrame(df_principal_mm_region)\n",
    "df_principal_mm_region.columns = ['P1','P2']\n",
    "\n",
    "df_principal_std_region = pca.fit_transform(df_normalized_std_region)\n",
    "df_principal_std_region = pd.DataFrame(df_principal_std_region)\n",
    "df_principal_std_region.columns = ['P1','P2']\n",
    "\n",
    "\n",
    "# Fit the data and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_principal_mm_group = pca.fit_transform(df_normalized_mm_group)\n",
    "df_principal_mm_group = pd.DataFrame(df_principal_mm_group)\n",
    "df_principal_mm_group.columns = ['P1','P2']\n",
    "\n",
    "df_principal_std_group = pca.fit_transform(df_normalized_std_group)\n",
    "df_principal_std_group = pd.DataFrame(df_principal_std_group)\n",
    "df_principal_std_group.columns = ['P1','P2']\n",
    "\n",
    "\n",
    "# Fit the data and Make the DataFrame for each of Scaler\n",
    "\n",
    "df_principal_mm_unit = pca.fit_transform(df_normalized_mm_unit)\n",
    "df_principal_mm_unit = pd.DataFrame(df_principal_mm_unit)\n",
    "df_principal_mm_unit.columns = ['P1','P2']\n",
    "\n",
    "df_principal_std_unit = pca.fit_transform(df_normalized_std_unit)\n",
    "df_principal_std_unit = pd.DataFrame(df_principal_std_unit)\n",
    "df_principal_std_unit.columns = ['P1','P2']\n",
    "\n",
    "\n",
    "Show_DBSCAN(df_principal_mm_region)\n",
    "Show_KMeans(df_principal_mm_region)\n",
    "Show_GMM(df_principal_mm_region)\n",
    "\n",
    "Show_DBSCAN(df_principal_std_region)\n",
    "Show_KMeans(df_principal_std_region)\n",
    "Show_GMM(df_principal_std_region)\n",
    "\n",
    "Show_DBSCAN(df_principal_mm_group)\n",
    "Show_KMeans(df_principal_mm_group)\n",
    "Show_GMM(df_principal_mm_group)\n",
    "\n",
    "Show_DBSCAN(df_principal_std_group)\n",
    "Show_KMeans(df_principal_std_group)\n",
    "Show_GMM(df_principal_std_group)\n",
    "\n",
    "Show_DBSCAN(df_principal_mm_unit)\n",
    "Show_KMeans(df_principal_mm_unit)\n",
    "Show_GMM(df_principal_mm_unit)\n",
    "\n",
    "Show_DBSCAN(df_principal_std_unit)\n",
    "Show_KMeans(df_principal_std_unit)\n",
    "Show_GMM(df_principal_std_unit)\n",
    "\n",
    "\n",
    "# 실루엣 계수는 -1 에서 1사이의 값을 가지며, \n",
    "# 1로 가까워 질수록 근처의 군집과 더 멀리 떨어져 있다는 것이고, \n",
    "# 0에 가까울 수록 근처의 군집과 가까워 진다는 것이다.\n",
    "# - 값은 아예 다른 군집에 데이터 포인트가 할당 됐음을 뜻한다.\n",
    "\n",
    "# 눈으로 먼저 몇개를 고른다음에 고른 것 중에서 실루엣 계수를 그린다음에 마무리 해야할 것 같아요.\n",
    "# purity는 정답 label이 없기에 쓸 수 없는것 같습니다.\n",
    "\n",
    "#dbscan = DBSCAN(eps = 0.05, min_samples = 10).fit(df_principal_mm_region)\n",
    "#labels_dbscan = dbscan.labels_\n",
    "#sil = dbscan.fit_predict(df_principal_mm_region)\n",
    "\n",
    "#plotSilhouette(df_principal_mm_region, sil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
